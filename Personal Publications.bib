
@article{gonnermann-muller_value_2024,
	title = {Value by {Design}: {Reducing} {Cognitive} {Load} by {Using} {Visual} {Guidance} in {Augmented} {Reality} - {An} {Eye}-{Tracking} {Study}},
	shorttitle = {Value by {Design}},
	url = {https://aisel.aisnet.org/icis2024/humtechinter/humtechinter/18},
	journal = {ICIS 2024 Proceedings},
	author = {Gonnermann-Müller, Jana and Leins, Nicolas and Gronau, Norbert and Kosch, Thomas},
	month = dec,
	year = {2024},
	file = {AIS Electronic Library (AISeL) - ICIS 2024 Proceedings\: Value by Design\: Reducing Cognitive Load by Using Visual Guidance in Augmented Reality - An Eye-Tracking Study:/Users/nleins/Zotero/storage/K4U9APXX/18.html:text/html;Gonnermann-Müller et al. - 2024 - Value by Design Reducing Cognitive Load by Using .pdf:/Users/nleins/Zotero/storage/8VZ7LH6M/Gonnermann-Müller et al. - 2024 - Value by Design Reducing Cognitive Load by Using .pdf:application/pdf},
}

@article{leins_comparing_2024,
	title = {Comparing head-mounted and handheld augmented reality for guided assembly},
	issn = {1783-8738},
	url = {https://doi.org/10.1007/s12193-024-00440-1},
	doi = {10.1007/s12193-024-00440-1},
	abstract = {Different Augmented Reality (AR) displays are becoming more commonly used for work since AR promises benefits by offering support, e.g., with additional information or hints. However, most research compares AR with traditional work support, like paper-based or web-based instructions. Since various AR technologies offer device-specific advantages and disadvantages, different AR technologies are more or less suitable to offer support without overwhelming or distracting the worker. Research, therefore, needs to derive empirical results from comparing different AR displays to derive concrete recommendations for action on the use and design of AR for specific contexts. To address this research gap, this experimental study investigates the effect of video-see-through head-mounted AR (Varjo XR-3) vs. handheld AR (Apple iPad) on performance (time and committed failure), motivation, and cognitive load for guided assembly. The study results reveal that both AR displays can successfully guide people in guided assembly tasks. On a descriptive level, the head-mounted AR device reveals slightly better results in terms of time and committed failures. Notably, the impact of technical restrictions on the study results was still evident. Accordingly, further investigation of device-specific differences is of continuing importance.},
	language = {en},
	urldate = {2024-10-01},
	journal = {Journal on Multimodal User Interfaces},
	author = {Leins, Nicolas and Gonnermann-Müller, Jana and Teichmann, Malte},
	month = sep,
	year = {2024},
	keywords = {Augmented reality, Assembly, Artificial Intelligence, Cognitive load, Handheld display, Head-mounted display},
	file = {Full Text PDF:/Users/nleins/Zotero/storage/5KPVXX7L/Leins et al. - 2024 - Comparing head-mounted and handheld augmented real.pdf:application/pdf},
}

@misc{leins_ur5e_2025,
	title = {{UR5e} {Augmented} {Reality} {Interface} on {Meta} {Quest} 3},
	url = {https://github.com/NLeins/UR5e-Augmented-Reality-Quest3-Interface},
	urldate = {2025-08-25},
	author = {Leins, Nicolas},
	year = {2025},
	file = {Snapshot:/Users/nleins/Zotero/storage/KTFGFRDF/UR5e-Augmented-Reality-Quest3-Interface.html:text/html},
}

@misc{haase_within-model_2026,
	title = {Within-{Model} vs {Between}-{Prompt} {Variability} in {Large} {Language} {Models} for {Creative} {Tasks}},
	url = {http://arxiv.org/abs/2601.21339},
	doi = {10.48550/arXiv.2601.21339},
	abstract = {How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43\% of variance, comparable to model choice (40.94\%). But for output quantity (fluency), model choice (51.25\%) and within-LLM variance (33.70\%) dominate, with prompts explaining only 4.22\%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34\%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.},
	urldate = {2026-02-02},
	publisher = {arXiv},
	author = {Haase, Jennifer and Gonnermann-Müller, Jana and Hanel, Paul H. P. and Leins, Nicolas and Kosch, Thomas and Mendling, Jan and Pokutta, Sebastian},
	month = jan,
	year = {2026},
	note = {arXiv:2601.21339 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/nleins/Zotero/storage/759M8QN8/Haase et al. - 2026 - Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks.pdf:application/pdf;Snapshot:/Users/nleins/Zotero/storage/VAJSYCG2/2601.html:text/html},
}

@misc{gonnermann-muller_facet_2026,
	title = {{FACET}: {Multi}-{Agent} {AI} {Supporting} {Teachers} in {Scaling} {Differentiated} {Learning} for {Diverse} {Students}},
	shorttitle = {{FACET}},
	url = {http://arxiv.org/abs/2601.22788},
	doi = {10.48550/arXiv.2601.22788},
	abstract = {Classrooms are becoming increasingly heterogeneous, comprising learners with diverse performance and motivation levels, language proficiencies, and learning differences such as dyslexia and ADHD. While teachers recognize the need for differentiated instruction, growing workloads create substantial barriers, making differentiated instruction an ideal that is often unrealized in practice. Current AI educational tools, which promise differentiated materials, are predominantly student-facing and performance-centric, ignoring other aspects that shape learning outcomes. We introduce FACET, a teacher-facing multi-agent framework designed to address these gaps by supporting differentiation that accounts for motivation, performance, and learning differences. Developed with educational stakeholders from the outset, the framework coordinates four specialized agents, including learner simulation, diagnostic assessment, material generation, and evaluation within a teacher-in-the-loop design. School principals (N = 30) shaped system requirements through participatory workshops, while in-service K-12 teachers (N = 70) evaluated material quality. Mixed-methods evaluation demonstrates strong perceived value for inclusive differentiation. Practitioners emphasized both the urgent need arising from classroom heterogeneity and the importance of maintaining pedagogical autonomy as a prerequisite for adoption. We discuss implications for future school deployment and outline partnerships for longitudinal classroom implementation.},
	urldate = {2026-02-03},
	publisher = {arXiv},
	author = {Gonnermann-Müller, Jana and Haase, Jennifer and Leins, Nicolas and Igel, Moritz and Fackeldey, Konstantin and Pokutta, Sebastian},
	month = jan,
	year = {2026},
	note = {arXiv:2601.22788 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {Preprint PDF:/Users/nleins/Zotero/storage/DEC5HR6F/Gonnermann-Müller et al. - 2026 - FACET Multi-Agent AI Supporting Teachers in Scaling Differentiated Learning for Diverse Students.pdf:application/pdf;Snapshot:/Users/nleins/Zotero/storage/WYK9WV96/2601.html:text/html},
}

@misc{leins_investigating_2026,
	title = {Investigating the {Influence} of {Spatial} {Ability} in {Augmented} {Reality}-assisted {Robot} {Programming}},
	url = {http://arxiv.org/abs/2602.03544},
	doi = {10.48550/arXiv.2602.03544},
	abstract = {Augmented Reality (AR) offers promising opportunities to enhance learning, but its mechanisms and effects are not yet fully understood. As learning becomes increasingly personalized, considering individual learner characteristics becomes more important. This study investigates the moderating effect of spatial ability on learning experience with AR in the context of robot programming. A between-subjects experiment (\$N=71\$) compared conventional robot programming to an AR-assisted approach using a head-mounted display. Participants' spatial ability was assessed using the Mental Rotation Test. The learning experience was measured through the System Usability Scale (SUS) and cognitive load. The results indicate that AR support does not significantly improve the learning experience compared to the conventional approach. However, AR appears to have a compensatory effect on the influence of spatial ability. In the control group, spatial ability was significantly positively associated with SUS scores and negatively associated with extraneous cognitive load, indicating that higher spatial ability predicts a better learning experience. In the AR condition, these relationships were not observable, suggesting that AR mitigated the disadvantage typically experienced by learners with lower spatial abilities. These findings suggest that AR can serve a compensatory function by reducing the influence of learner characteristics. Future research should further explore this compensatory role of AR to guide the design of personalized learning environments that address diverse learner needs and reduce barriers for learners with varying cognitive profiles.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Leins, Nicolas and Gonnermann-Müller, Jana and Teichmann, Malte and Pokutta, Sebastian},
	month = feb,
	year = {2026},
	note = {arXiv:2602.03544 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Robotics},
	file = {Preprint PDF:/Users/nleins/Zotero/storage/Y7B2F593/Leins et al. - 2026 - Investigating the Influence of Spatial Ability in Augmented Reality-assisted Robot Programming.pdf:application/pdf;Snapshot:/Users/nleins/Zotero/storage/MBY7EJEA/2602.html:text/html},
}

@misc{gonnermann-muller_stable_2026,
	title = {Stable {Personas}: {Dual}-{Assessment} of {Temporal} {Stability} in {LLM}-{Based} {Human} {Simulation}},
	shorttitle = {Stable {Personas}},
	url = {http://arxiv.org/abs/2601.22812},
	doi = {10.48550/arXiv.2601.22812},
	abstract = {Large Language Models (LLMs) acting as artificial agents offer the potential for scalable behavioral research, yet their validity depends on whether LLMs can maintain stable personas across extended conversations. We address this point using a dual-assessment framework measuring both self-reported characteristics and observer-rated persona expression. Across two experiments testing four persona conditions (default, high, moderate, and low ADHD presentations), seven LLMs, and three semantically equivalent persona prompts, we examine between-conversation stability (3,473 conversations) and within-conversation stability (1,370 conversations and 18 turns). Self-reports remain highly stable both between and within conversations. However, observer ratings reveal a tendency for persona expressions to decline during extended conversations. These findings suggest that persona-instructed LLMs produce stable, persona-aligned self-reports, an important prerequisite for behavioral research, while identifying this regression tendency as a boundary condition for multi-agent social simulation.},
	urldate = {2026-02-09},
	publisher = {arXiv},
	author = {Gonnermann-Müller, Jana and Haase, Jennifer and Leins, Nicolas and Kosch, Thomas and Pokutta, Sebastian},
	month = jan,
	year = {2026},
	note = {arXiv:2601.22812 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {Preprint PDF:/Users/nleins/Zotero/storage/APMQ3LX3/Gonnermann-Müller et al. - 2026 - Stable Personas Dual-Assessment of Temporal Stability in LLM-Based Human Simulation.pdf:application/pdf;Snapshot:/Users/nleins/Zotero/storage/72GG7AJ8/2601.html:text/html},
}
